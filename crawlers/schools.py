import time
import os
import hashlib
import hmac
import base64
import json
from .base import BaseCrawler

class SchoolCrawler(BaseCrawler):
    
    def generate_signsafe(self, params):
        """ç”Ÿæˆsignsafeç­¾å"""
        secret = "D23ABC@#56"
        sorted_keys = sorted(params.keys())
        query_string = '&'.join([f"{k}={params[k]}" for k in sorted_keys])
        sign_string = f"api-gaokao.zjzw.cn/apidata/web?{query_string}"
        
        hmac_result = hmac.new(
            secret.encode('utf-8'),
            sign_string.encode('utf-8'),
            hashlib.sha1
        ).digest()
        
        base64_result = base64.b64encode(hmac_result).decode('utf-8')
        final_signature = hashlib.md5(base64_result.encode('utf-8')).hexdigest()
        
        return final_signature
    
    def get_school_detail(self, school_id):
        """è·å–å­¦æ ¡è¯¦ç»†ä¿¡æ¯ï¼ˆé€šè¿‡APIï¼‰"""
        payload = {
            "school_id": school_id,
            "uri": "apidata/api/gkv3/school/detail"
        }
        
        data = self.make_request(payload, retry=2)
        
        # è°ƒè¯•è¾“å‡º - æ‰“å°APIè¿”å›çš„æ‰€æœ‰å­—æ®µ
        if data and 'data' in data and isinstance(data['data'], dict) and not hasattr(self, '_detail_debug_printed'):
            print(f"\nğŸ” APIè¯¦æƒ…æ¥å£è¿”å›çš„å­—æ®µ:")
            print(f"   å­—æ®µåˆ—è¡¨: {list(data['data'].keys())}")
            # æ£€æŸ¥æ˜¯å¦æœ‰contentç›¸å…³å­—æ®µ
            for key in data['data'].keys():
                if 'content' in key.lower() or 'intro' in key.lower() or 'desc' in key.lower():
                    value = data['data'][key]
                    preview = str(value)[:150] if value else "ç©º"
                    print(f"   >>> å‘ç° '{key}': {preview}...")
            self._detail_debug_printed = True
        
        if data and 'data' in data and isinstance(data['data'], dict):
            return data['data']
        return None
    
    def get_school_static_info(self, school_id):
        """è·å–å­¦æ ¡å®Œæ•´é™æ€ä¿¡æ¯ï¼ˆåŒ…å«ä»‹ç»ã€é‚®ç®±ç­‰ï¼‰"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„URL
        urls = [
            f"https://static-data.gaokao.cn/www/2.0/school/{school_id}/info.json",
            f"https://static-data.gaokao.cn/www/2.0/school/{school_id}/info.json?a=www.gaokao.cn",
        ]
        
        for url_idx, url in enumerate(urls):
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    result = response.json()
                    
                    # è°ƒè¯•è¾“å‡º - æ‰“å°é™æ€æ¥å£è¿”å›çš„æ‰€æœ‰æ•°æ®
                    if not hasattr(self, '_static_debug_printed'):
                        print(f"\nğŸ” é™æ€æ¥å£ URL{url_idx+1} è¿”å›æ•°æ®:")
                        print(f"   å®Œæ•´å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}...\n")
                        
                        if result.get('code') == 0 and 'data' in result:
                            print(f"   dataå­—æ®µç±»å‹: {type(result['data'])}")
                            if isinstance(result['data'], dict):
                                print(f"   æ‰€æœ‰å­—æ®µ: {list(result['data'].keys())}")
                                # æŸ¥æ‰¾åŒ…å«content/introçš„å­—æ®µ
                                for key, value in result['data'].items():
                                    if 'content' in key.lower() or 'intro' in key.lower() or 'desc' in key.lower():
                                        preview = str(value)[:150] if value else "ç©º"
                                        print(f"   >>> å‘ç° '{key}': {preview}...")
                            elif isinstance(result['data'], list):
                                print(f"   dataæ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(result['data'])}")
                        else:
                            print(f"   é”™è¯¯ç : {result.get('code')}, æ¶ˆæ¯: {result.get('message')}")
                        
                        self._static_debug_printed = True
                    
                    if result.get('code') == 0 and 'data' in result:
                        return result['data']
                        
            except Exception as e:
                print(f"âš ï¸  URL{url_idx+1} è·å–å¤±è´¥: {str(e)}")
        
        return None
    
    def get_school_content_alternative(self, school_id):
        """å°è¯•å…¶ä»–å¯èƒ½çš„æ¥å£è·å–å­¦æ ¡ä»‹ç»"""
        alternative_urls = [
            f"https://static-data.gaokao.cn/www/2.0/schoolSpecial/{school_id}/pc_special.json",
            f"https://static-data.gaokao.cn/www/2.0/schoolInfo/{school_id}/info.json",
        ]
        
        for url in alternative_urls:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    result = response.json()
                    
                    # è°ƒè¯•è¾“å‡º
                    if not hasattr(self, '_alt_debug_printed'):
                        print(f"\nğŸ” å¤‡ç”¨æ¥å£ {url} è¿”å›:")
                        print(f"   å“åº”ç‰‡æ®µ: {json.dumps(result, ensure_ascii=False)[:300]}...\n")
                        self._alt_debug_printed = True
                    
                    if result.get('code') == 0 and 'data' in result:
                        data = result['data']
                        if isinstance(data, dict):
                            # æŸ¥æ‰¾contentç›¸å…³å­—æ®µ
                            for key in ['content', 'intro', 'introduction', 'school_intro', 'description']:
                                if key in data and data[key]:
                                    return data[key]
            except:
                continue
        
        return None

    def get_enhanced_school_list(self, page=1, size=20):
        """è·å–å¢å¼ºç‰ˆå­¦æ ¡åˆ—è¡¨"""
        base_url = "https://api-gaokao.zjzw.cn/apidata/web"
        cookie = os.getenv('GAOKAO_COOKIE', '')
        
        # æ„å»ºå‚æ•°
        params = {
            "autosign": "",
            "keyword": "",
            "local_type_id": "2073",
            "page": str(page),
            "platform": "2",
            "province_id": "",
            "ranktype": "",
            "request_type": "1",
            "size": str(size),
            "spe_ids": "",
            "top_school_id": "",
            "uri": "v1/school/lists"
        }
        
        signsafe = self.generate_signsafe(params)
        
        # æ„å»ºURL
        query_string = '&'.join([f"{k}={params[k]}" for k in sorted(params.keys())])
        full_url = f"{base_url}?{query_string}&signsafe={signsafe}"
        
        # POST bodyï¼ˆæ•°å­—ç±»å‹ï¼‰
        post_body = {
            "autosign": "",
            "keyword": "",
            "local_type_id": 2073,
            "page": int(page),
            "platform": "2",
            "province_id": "",
            "ranktype": "",
            "request_type": 1,
            "signsafe": signsafe,
            "size": int(size),
            "spe_ids": "",
            "top_school_id": "",
            "uri": "v1/school/lists"
        }
        
        headers = self.headers.copy()
        if cookie:
            headers["cookie"] = cookie
        
        try:
            response = self.session.post(
                full_url,
                headers=headers,
                data=json.dumps(post_body),
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    return result
                elif result.get('code') == 1010001 and not cookie:
                    print(f"âš ï¸  å¢å¼ºAPIéœ€è¦Cookieè®¤è¯")
                else:
                    print(f"âš ï¸  APIè¿”å›é”™è¯¯: code={result.get('code')}, message={result.get('message')}")
            
        except Exception as e:
            print(f"âš ï¸  å¢å¼ºæ•°æ®è¯·æ±‚å¤±è´¥: {str(e)}")
        
        return None
    
    def merge_enhanced_data(self, schools_basic, max_pages=10):
        """å°†å¢å¼ºæ•°æ®åˆå¹¶åˆ°åŸºç¡€å­¦æ ¡åˆ—è¡¨"""
        enhanced_dict = {}
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è·å–å¢å¼ºç‰ˆå­¦æ ¡æ•°æ®...")
        print(f"{'='*60}\n")
        
        page = 1
        total_fetched = 0
        
        while page <= max_pages:
            enhanced_data = self.get_enhanced_school_list(page=page, size=20)
            
            if enhanced_data and enhanced_data.get('code') == 0:
                items = enhanced_data.get('data', {}).get('item', [])
                
                if not items:
                    print(f"âœ“ å¢å¼ºæ•°æ®ç¬¬ {page} é¡µæ— æ•°æ®")
                    break
                
                for item in items:
                    school_id = item.get('school_id')
                    if school_id:
                        enhanced_dict[school_id] = {
                            'label_list': item.get('label_list', []),
                            'recommend_master_level': item.get('recommend_master_level'),
                            'is_top': item.get('is_top'),
                            'attr_list': item.get('attr_list', []),
                            'hightitle': item.get('hightitle')
                        }
                        total_fetched += 1
                
                print(f"âœ“ å¢å¼ºæ•°æ®ç¬¬ {page} é¡µï¼šè·å– {len(items)} æ‰€å­¦æ ¡ï¼ˆç´¯è®¡{total_fetched}æ‰€ï¼‰")
                page += 1
                self.polite_sleep(3.0, 6.0)
            else:
                if page == 1 and not os.getenv('GAOKAO_COOKIE'):
                    print("\nğŸ’¡ æç¤ºï¼šå¢å¼ºæ•°æ®éœ€è¦Cookie")
                    print("   1. è®¿é—® www.gaokao.cn å¹¶ç™»å½•")
                    print("   2. F12 æ§åˆ¶å°è¾“å…¥: document.cookie")
                    print("   3. è®¾ç½®ç¯å¢ƒå˜é‡: export GAOKAO_COOKIE='ä½ çš„cookie'\n")
                break
        
        # åˆå¹¶æ•°æ®
        merged_count = 0
        for school in schools_basic:
            school_id = school.get('school_id')
            if school_id and school_id in enhanced_dict:
                school.update(enhanced_dict[school_id])
                merged_count += 1
        
        if merged_count > 0:
            print(f"\nâœ“ æˆåŠŸåˆå¹¶ {merged_count}/{len(schools_basic)} æ‰€å­¦æ ¡çš„å¢å¼ºæ•°æ®\n")
        
        return schools_basic
    
    def crawl(self, max_pages=None, fetch_detail=True, fetch_enhanced=True, fetch_static_info=True):
        """çˆ¬å–å­¦æ ¡åˆ—è¡¨"""
        max_pages = max_pages or int(os.getenv('MAX_PAGES', '10'))
        fetch_detail = os.getenv('FETCH_DETAIL', str(fetch_detail)).lower() == 'true'
        fetch_enhanced = os.getenv('FETCH_ENHANCED', str(fetch_enhanced)).lower() == 'true'
        fetch_static_info = os.getenv('FETCH_STATIC_INFO', str(fetch_static_info)).lower() == 'true'
        
        schools = []
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–å­¦æ ¡åˆ—è¡¨ï¼ˆæœ€å¤š {max_pages} é¡µï¼‰")
        print(f"è¯¦ç»†ä¿¡æ¯: {'âœ“' if fetch_detail else 'âœ—'} | "
              f"å¢å¼ºæ•°æ®: {'âœ“' if fetch_enhanced else 'âœ—'} | "
              f"å®Œæ•´ä¿¡æ¯: {'âœ“' if fetch_static_info else 'âœ—'}")
        print(f"{'='*60}\n")
        
        for page in range(1, max_pages + 1):
            payload = {
                "keyword": "",
                "page": page,
                "province_id": "",
                "ranktype": "",
                "request_type": 1,
                "size": 20,
                "type": "",
                "uri": "apidata/api/gkv3/school/lists"
            }
            
            data = self.make_request(payload)
            
            if not data or 'data' not in data or 'item' not in data['data']:
                print(f"âœ— ç¬¬ {page} é¡µï¼šè¯·æ±‚å¤±è´¥")
                break
            
            items = data['data']['item']
            if not items:
                print(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break
            
            for item in items:
                school_id = item.get('school_id')
                
                school_info = {
                    'school_id': school_id,
                    'name': item.get('name'),
                    'province': item.get('province_name'),
                    'city': item.get('city_name'),
                    'county': item.get('county_name'),
                    'type': item.get('type_name'),
                    'level': item.get('level_name'),
                    'belong': item.get('belong'),
                    'rank': item.get('rank'),
                    'dual_class': item.get('dual_class_name'),
                    'f985': item.get('f985'),
                    'f211': item.get('f211'),
                    'is_dual_class': item.get('dual_class'),
                    'nature': item.get('nature_name'),
                    'view_total': item.get('view_total'),
                }
                
                # è·å–APIè¯¦ç»†ä¿¡æ¯
                if fetch_detail and school_id:
                    detail = self.get_school_detail(school_id)
                    if detail:
                        school_info.update({
                            'logo': detail.get('logo'),
                            'img': detail.get('img'),
                            'address': detail.get('address'),
                            'phone': detail.get('phone'),
                            'email': detail.get('email'),
                            'website': detail.get('site'),
                        })
                        # æ£€æŸ¥detailä¸­æ˜¯å¦æœ‰content
                        if 'content' in detail:
                            school_info['content'] = detail['content']
                        if 'intro' in detail:
                            school_info['intro'] = detail['intro']
                        
                        self.polite_sleep(1.0, 2.0)
                
                # è·å–é™æ€å®Œæ•´ä¿¡æ¯
                if fetch_static_info and school_id:
                    static_info = self.get_school_static_info(school_id)
                    if static_info:
                        # å°è¯•å„ç§å¯èƒ½çš„å­—æ®µå
                        for content_key in ['content', 'intro', 'introduction', 'school_intro', 'description']:
                            if content_key in static_info:
                                school_info[content_key] = static_info[content_key]
                        
                        # å…¶ä»–å­—æ®µ
                        school_info.update({
                            'central': static_info.get('central'),
                            'school_site': static_info.get('school_site') or static_info.get('site'),
                            'emails': static_info.get('emails') or static_info.get('email'),
                            'colleges_level': static_info.get('colleges_level'),
                            'old_name': static_info.get('old_name'),
                            'create_year': static_info.get('create_date') or static_info.get('create_year'),
                            'province_id': static_info.get('province_id'),
                            'city_id': static_info.get('city_id'),
                            'town': static_info.get('town_name') or static_info.get('town'),
                            'level_name': static_info.get('level_name'),
                            'department': static_info.get('department') or static_info.get('belong'),
                        })
                        
                        # å¦‚æœè¿˜æ²¡æœ‰contentï¼Œå°è¯•å¤‡ç”¨æ¥å£
                        if not school_info.get('content') and not school_info.get('intro'):
                            alt_content = self.get_school_content_alternative(school_id)
                            if alt_content:
                                school_info['content'] = alt_content
                        
                        self.polite_sleep(2.0, 4.0)
                
                schools.append(school_info)
            
            info_str = ""
            if fetch_detail:
                info_str += "(å«è¯¦æƒ…)"
            if fetch_static_info:
                info_str += "(å«å®Œæ•´ä¿¡æ¯)"
            
            print(f"âœ“ ç¬¬ {page} é¡µï¼šè·å– {len(items)} æ‰€å­¦æ ¡ {info_str}")
            self.polite_sleep(3.0, 6.0)
        
        # åˆå¹¶å¢å¼ºæ•°æ®
        if fetch_enhanced and schools:
            enhanced_pages = max(max_pages, (len(schools) // 20) + 2)
            schools = self.merge_enhanced_data(schools, max_pages=enhanced_pages)
        
        # æœ€ç»ˆè¾“å‡º - æ˜¾ç¤ºç¬¬ä¸€æ‰€å­¦æ ¡çš„å®Œæ•´æ•°æ®ç”¨äºè°ƒè¯•
        if schools:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š ç¬¬ä¸€æ‰€å­¦æ ¡çš„å®Œæ•´æ•°æ®ï¼ˆç”¨äºæ£€æŸ¥ï¼‰:")
            print(f"{'='*60}")
            print(json.dumps(schools[0], ensure_ascii=False, indent=2))
            print(f"{'='*60}\n")
        
        self.save_to_json(schools, 'schools.json')
        print(f"\n{'='*60}")
        print(f"å­¦æ ¡çˆ¬å–å®Œæˆï¼å…± {len(schools)} æ‰€")
        print(f"{'='*60}\n")
        
        return schools

if __name__ == "__main__":
    import sys
    
    max_pages = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    fetch_detail = sys.argv[2].lower() == 'true' if len(sys.argv) > 2 else True
    fetch_enhanced = sys.argv[3].lower() == 'true' if len(sys.argv) > 3 else True
    fetch_static_info = sys.argv[4].lower() == 'true' if len(sys.argv) > 4 else True
    
    crawler = SchoolCrawler()
    crawler.crawl(
        max_pages=max_pages, 
        fetch_detail=fetch_detail, 
        fetch_enhanced=fetch_enhanced,
        fetch_static_info=fetch_static_info
    )

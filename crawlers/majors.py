import time
import json
from .base import BaseCrawler

class MajorCrawler(BaseCrawler):
    def crawl(self, max_pages=200):
        """çˆ¬å–ä¸“ä¸šåˆ—è¡¨"""
        majors = []
        page = 1
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–ä¸“ä¸šåˆ—è¡¨")
        print(f"{'='*60}\n")
        
        while page <= max_pages:
            payload = {
                "keyword": "",
                "page": page,
                "size": 30,
                "level1": "",
                "level2": "",
                "level3": "",
                "uri": "apidata/api/gkv3/special/lists"
            }
            
            data = self.make_request(payload)
            
            if not data or 'data' not in data:
                print(f"âœ— ç¬¬ {page} é¡µï¼šè¯·æ±‚å¤±è´¥")
                if page == 1:
                    print("âš ï¸  API å¯èƒ½å·²æ›´æ”¹ï¼Œè¯·æ£€æŸ¥å‚æ•°")
                break
            
            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
            data_content = data['data']
            
            # å¦‚æœ data æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            if isinstance(data_content, str):
                try:
                    data_content = json.loads(data_content)
                except Exception as e:
                    print(f"âœ— ç¬¬ {page} é¡µï¼šæ•°æ®è§£æå¤±è´¥ - {str(e)}")
                    break
            
            # æå– items
            items = []
            if isinstance(data_content, dict):
                items = data_content.get('item') or data_content.get('items') or []
            elif isinstance(data_content, list):
                items = data_content
            
            if not items:
                print(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break
            
            for item in items:
                major_info = {
                    'special_id': item.get('special_id') or item.get('id'),
                    'code': item.get('code') or item.get('special_code'),
                    'name': item.get('name') or item.get('special_name'),
                    'level1_name': item.get('level1_name'),
                    'level2_name': item.get('level2_name'),
                    'level3_name': item.get('level3_name'),
                    'degree': item.get('degree'),
                    'years': item.get('years') or item.get('limit_year')
                }
                majors.append(major_info)
            
            print(f"âœ“ ç¬¬ {page} é¡µï¼šè·å– {len(items)} ä¸ªä¸“ä¸š")
            
            # æ¯10é¡µæ˜¾ç¤ºè¿›åº¦
            if page % 10 == 0:
                print(f"ğŸ“Š è¿›åº¦ï¼šå·²çˆ¬å– {len(majors)} ä¸ªä¸“ä¸š...")
            
            page += 1
            self.polite_sleep()
        
        self.save_to_json(majors, 'majors.json')
        print(f"\n{'='*60}")
        print(f"ä¸“ä¸šçˆ¬å–å®Œæˆï¼å…± {len(majors)} ä¸ª")
        print(f"{'='*60}\n")
        
        return majors

if __name__ == "__main__":
    crawler = MajorCrawler()
    crawler.crawl()

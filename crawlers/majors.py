import time
import json
import os
from .base import BaseCrawler

class MajorCrawler(BaseCrawler):
    def crawl(self, max_pages=200):
        """çˆ¬å–ä¸“ä¸šåˆ—è¡¨"""
        majors = []
        page = 1
        debug = os.getenv('DEBUG_MODE', 'false').lower() == 'true'
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–ä¸“ä¸šåˆ—è¡¨")
        if debug:
            print(f"ğŸ” è°ƒè¯•æ¨¡å¼å·²å¼€å¯")
        print(f"{'='*60}\n")
        
        while page <= max_pages:
            payload = {
                "keyword": "",
                "page": page,
                "size": 30,
                "level1": "",
                "level2": "",
                "level3": "",
                "uri": "apidata/api/gkv3/special/lists"
            }
            
            data = self.make_request(payload)
            
            if not data or 'data' not in data:
                print(f"âœ— ç¬¬ {page} é¡µï¼šè¯·æ±‚å¤±è´¥")
                if page == 1:
                    print("âš ï¸  API å¯èƒ½å·²æ›´æ”¹ï¼Œè¯·æ£€æŸ¥å‚æ•°")
                break
            
            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
            data_content = data['data']
            
            # å¦‚æœ data æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            if isinstance(data_content, str):
                if debug or page >= 10:  # åœ¨é—®é¢˜é¡µæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                    print(f"\nğŸ” ç¬¬ {page} é¡µè°ƒè¯•ä¿¡æ¯:")
                    print(f"   dataç±»å‹: str")
                    print(f"   dataé•¿åº¦: {len(data_content)}")
                    print(f"   dataå‰100å­—ç¬¦: {data_content[:100]}")
                    print(f"   dataå100å­—ç¬¦: {data_content[-100:]}")
                
                try:
                    data_content = json.loads(data_content)
                    if debug or page >= 10:
                        print(f"   âœ“ JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    print(f"âœ— ç¬¬ {page} é¡µï¼šJSONè§£æå¤±è´¥ - {str(e)}")
                    print(f"   é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œ ç¬¬{e.colno}åˆ—")
                    print(f"   åŸå§‹å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {data_content[:200]}")
                    print(f"   åŸå§‹å†…å®¹ï¼ˆå200å­—ç¬¦ï¼‰: {data_content[-200:]}")
                    
                    # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜
                    # 1. å»é™¤ BOM
                    data_content = data_content.strip('\ufeff')
                    # 2. å»é™¤å‰åç©ºç™½
                    data_content = data_content.strip()
                    
                    try:
                        data_content = json.loads(data_content)
                        print(f"   âœ“ ä¿®å¤åè§£ææˆåŠŸ")
                    except:
                        print(f"   âœ— ä¿®å¤å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ")
                        break
                except Exception as e:
                    print(f"âœ— ç¬¬ {page} é¡µï¼šæ•°æ®è§£æå¤±è´¥ - {str(e)}")
                    break
            
            # æå– items
            items = []
            if isinstance(data_content, dict):
                items = data_content.get('item') or data_content.get('items') or []
                if debug or (page >= 10 and not items):
                    print(f"   data_contentç±»å‹: dict")
                    print(f"   å¯ç”¨å­—æ®µ: {list(data_content.keys())}")
            elif isinstance(data_content, list):
                items = data_content
                if debug or page >= 10:
                    print(f"   data_contentç±»å‹: list, é•¿åº¦: {len(items)}")
            else:
                if debug or page >= 10:
                    print(f"   âš ï¸  æœªçŸ¥çš„data_contentç±»å‹: {type(data_content)}")
            
            if not items:
                print(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break
            
            for item in items:
                major_info = {
                    'special_id': item.get('special_id') or item.get('id'),
                    'code': item.get('code') or item.get('special_code'),
                    'name': item.get('name') or item.get('special_name'),
                    'level1_name': item.get('level1_name'),
                    'level2_name': item.get('level2_name'),
                    'level3_name': item.get('level3_name'),
                    'degree': item.get('degree'),
                    'years': item.get('years') or item.get('limit_year')
                }
                majors.append(major_info)
            
            print(f"âœ“ ç¬¬ {page} é¡µï¼šè·å– {len(items)} ä¸ªä¸“ä¸š")
            
            # æ¯10é¡µæ˜¾ç¤ºè¿›åº¦
            if page % 10 == 0:
                print(f"ğŸ“Š è¿›åº¦ï¼šå·²çˆ¬å– {len(majors)} ä¸ªä¸“ä¸š...")
            
            page += 1
            self.polite_sleep()
        
        self.save_to_json(majors, 'majors.json')
        print(f"\n{'='*60}")
        print(f"ä¸“ä¸šçˆ¬å–å®Œæˆï¼å…± {len(majors)} ä¸ª")
        print(f"{'='*60}\n")
        
        return majors

if __name__ == "__main__":
    crawler = MajorCrawler()
    crawler.crawl()

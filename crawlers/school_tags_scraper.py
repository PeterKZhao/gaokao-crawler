import requests
import time
import json
from bs4 import BeautifulSoup

class SchoolTagsScraper:
    """ä»æŒä¸Šé«˜è€ƒHTMLé¡µé¢çˆ¬å–æ‰€æœ‰å­¦æ ¡çš„æ ‡ç­¾"""
    
    def __init__(self):
        self.base_url = "https://www.gaokao.cn/school/search"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.gaokao.cn/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
    
    def scrape_page(self, page_num):
        """çˆ¬å–æŒ‡å®šé¡µçš„å­¦æ ¡æ ‡ç­¾"""
        url = f"{self.base_url}?page={page_num}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code != 200:
                print(f"âœ— ç¬¬{page_num}é¡µè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            school_items = soup.find_all('div', class_='school-search_schoolItem__3q7R2')
            
            if not school_items:
                return []
            
            schools = []
            for item in school_items:
                try:
                    # æå–å­¦æ ¡åç§°
                    name_elem = item.find('h3', class_='school-search_schoolName__1L7pc')
                    if not name_elem:
                        continue
                    
                    # æ¸…ç†åç§°ï¼ˆå¯èƒ½æœ‰<em>æ ‡ç­¾ï¼‰
                    school_name = name_elem.get_text().split('\n')[0].strip()
                    
                    # æå–æ ‡ç­¾
                    tags = []
                    tags_div = item.find('div', class_='school-search_tags__ZPsHs')
                    if tags_div:
                        tag_spans = tags_div.find_all('span')
                        tags = [tag.get_text().strip() for tag in tag_spans if tag.get_text().strip()]
                    
                    schools.append({
                        'name': school_name,
                        'tags': tags
                    })
                
                except Exception as e:
                    print(f"  è§£æå­¦æ ¡é¡¹å‡ºé”™: {e}")
                    continue
            
            print(f"âœ“ ç¬¬{page_num}é¡µ: {len(schools)}æ‰€å­¦æ ¡")
            return schools
        
        except Exception as e:
            print(f"âœ— ç¬¬{page_num}é¡µå‡ºé”™: {e}")
            return None
    
    def scrape_all(self, max_pages=150):
        """çˆ¬å–æ‰€æœ‰å­¦æ ¡çš„æ ‡ç­¾ï¼ˆçº¦150é¡µï¼‰"""
        all_tags = {}
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–å­¦æ ¡æ ‡ç­¾")
        print(f"{'='*60}\n")
        
        for page in range(1, max_pages + 1):
            schools = self.scrape_page(page)
            
            if schools is None:
                print(f"ç¬¬{page}é¡µå¤±è´¥ï¼Œé‡è¯•...")
                time.sleep(3)
                schools = self.scrape_page(page)
                if schools is None:
                    print(f"ç¬¬{page}é¡µé‡è¯•å¤±è´¥ï¼Œè·³è¿‡")
                    continue
            
            if not schools:
                print(f"ç¬¬{page}é¡µæ— æ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break
            
            # ä¿å­˜æ ‡ç­¾
            for school in schools:
                all_tags[school['name']] = school['tags']
            
            # æ¯10é¡µä¿å­˜ä¸€æ¬¡
            if page % 10 == 0:
                self.save_tags(all_tags, 'data/school_tags_temp.json')
                print(f"  ğŸ’¾ å·²ä¿å­˜ä¸´æ—¶æ•°æ®ï¼ˆ{len(all_tags)}æ‰€å­¦æ ¡ï¼‰\n")
            
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æœ€ç»ˆä¿å­˜
        self.save_tags(all_tags, 'data/school_tags.json')
        
        print(f"\n{'='*60}")
        print(f"âœ“ å®Œæˆï¼å…±çˆ¬å– {len(all_tags)} æ‰€å­¦æ ¡çš„æ ‡ç­¾")
        print(f"{'='*60}\n")
        
        return all_tags
    
    def save_tags(self, tags_dict, filepath):
        """ä¿å­˜æ ‡ç­¾å­—å…¸"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(tags_dict, f, ensure_ascii=False, indent=2)
        print(f"  å·²ä¿å­˜åˆ° {filepath}")

if __name__ == "__main__":
    import sys
    scraper = SchoolTagsScraper()
    
    # å…ˆæµ‹è¯•3é¡µ
    test_pages = int(sys.argv[1]) if len(sys.argv) > 1 else 3
    scraper.scrape_all(max_pages=test_pages)

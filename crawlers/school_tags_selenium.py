from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
import time
import json

class SchoolTagsSeleniumScraper:
    """ä½¿ç”¨Seleniumçˆ¬å–å­¦æ ¡æ ‡ç­¾"""
    
    def __init__(self, headless=True):
        options = Options()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.base_url = "https://www.gaokao.cn/school/search"
    
    def scrape_page(self, page_num):
        """çˆ¬å–æŒ‡å®šé¡µ"""
        url = f"{self.base_url}?page={page_num}"
        
        try:
            self.driver.get(url)
            
            # ç­‰å¾…å­¦æ ¡åˆ—è¡¨åŠ è½½
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "school-search_schoolItem__3q7R2"))
            )
            
            # ç­‰å¾…é¢å¤–æ—¶é—´ç¡®ä¿å†…å®¹åŠ è½½å®Œæˆ
            time.sleep(2)
            
            # è·å–æ‰€æœ‰å­¦æ ¡é¡¹
            school_items = self.driver.find_elements(By.CLASS_NAME, "school-search_schoolItem__3q7R2")
            
            schools = []
            for item in school_items:
                try:
                    # å­¦æ ¡åç§°
                    name_elem = item.find_element(By.CLASS_NAME, "school-search_schoolName__1L7pc")
                    school_name = name_elem.text.split('\n')[0].strip()
                    
                    # æ ‡ç­¾
                    tags = []
                    try:
                        tags_div = item.find_element(By.CLASS_NAME, "school-search_tags__ZPsHs")
                        tag_elements = tags_div.find_elements(By.TAG_NAME, "span")
                        tags = [tag.text.strip() for tag in tag_elements if tag.text.strip()]
                    except:
                        pass
                    
                    schools.append({
                        'name': school_name,
                        'tags': tags
                    })
                
                except Exception as e:
                    continue
            
            print(f"âœ“ ç¬¬{page_num}é¡µ: {len(schools)}æ‰€å­¦æ ¡")
            return schools
        
        except TimeoutException:
            print(f"âœ— ç¬¬{page_num}é¡µè¶…æ—¶")
            return None
        except Exception as e:
            print(f"âœ— ç¬¬{page_num}é¡µå‡ºé”™: {e}")
            return None
    
    def scrape_all(self, max_pages=150):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        all_tags = {}
        
        print(f"\n{'='*60}")
        print(f"ä½¿ç”¨Seleniumçˆ¬å–å­¦æ ¡æ ‡ç­¾")
        print(f"{'='*60}\n")
        
        for page in range(1, max_pages + 1):
            schools = self.scrape_page(page)
            
            if schools is None:
                print(f"ç¬¬{page}é¡µå¤±è´¥ï¼Œé‡è¯•...")
                time.sleep(5)
                schools = self.scrape_page(page)
                if schools is None:
                    continue
            
            if not schools:
                print(f"ç¬¬{page}é¡µæ— æ•°æ®ï¼Œå®Œæˆ")
                break
            
            for school in schools:
                all_tags[school['name']] = school['tags']
            
            # æ¯10é¡µä¿å­˜
            if page % 10 == 0:
                self.save_tags(all_tags, 'data/school_tags_temp.json')
                print(f"  ğŸ’¾ å·²ä¿å­˜ï¼ˆ{len(all_tags)}æ‰€å­¦æ ¡ï¼‰\n")
            
            time.sleep(3)
        
        self.save_tags(all_tags, 'data/school_tags.json')
        self.driver.quit()
        
        print(f"\n{'='*60}")
        print(f"âœ“ å®Œæˆï¼å…± {len(all_tags)} æ‰€å­¦æ ¡")
        print(f"{'='*60}\n")
        
        return all_tags
    
    def save_tags(self, tags_dict, filepath):
        """ä¿å­˜æ ‡ç­¾"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(tags_dict, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    import sys
    scraper = SchoolTagsSeleniumScraper(headless=True)
    
    test_pages = int(sys.argv[1]) if len(sys.argv) > 1 else 3
    scraper.scrape_all(max_pages=test_pages)
